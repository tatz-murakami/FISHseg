{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e0e8b2-e27a-4bf9-8a7c-16c93b36f96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "from cellpose import models, core\n",
    "\n",
    "use_GPU = core.use_gpu()\n",
    "print('>>> GPU activated? %d'%use_GPU)\n",
    "\n",
    "from skimage import io\n",
    "from skimage import exposure\n",
    "import tqdm\n",
    "import napari\n",
    "import pandas as pd\n",
    "\n",
    "import zarr\n",
    "from dask import array as da\n",
    "\n",
    "from ome_zarr.io import parse_url\n",
    "from ome_zarr.reader import Reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b61d89-a953-484b-a4cd-a2c10b13734d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalization_two_values(arr, lower, upper):\n",
    "    \"\"\"\n",
    "    Normalize array so that the lower values to be 0 and upper values to be 1.\n",
    "    \"\"\"\n",
    "    return (arr - lower) / (upper - lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d094e22-e6ad-40ae-8600-fc3752461f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to dataset and model\n",
    "dataset_folder = \"/mnt/ampa02_data01/gabacoll/shared/Yuchen/model_training/crops\"\n",
    "aug_folder = os.path.join(dataset_folder, 'augment')\n",
    "train_folder = os.path.join(aug_folder,'training')\n",
    "models_path = os.path.join(train_folder,'models')\n",
    "\n",
    "models_file = os.listdir(models_path); models_file.sort()\n",
    "model_path = os.path.join(train_folder,'models',models_file[-1])\n",
    "\n",
    "model = models.CellposeModel(gpu=use_GPU, pretrained_model=model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d1ccaa-d4ff-4457-8b5e-ed7f8bac48fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "### parameters\n",
    "data_path = '/mnt/ampa02_data01/tmurakami/240417_whole_4color_1st_M037-3pb/fused/fused.n5' # zarr with pyramid resolution\n",
    "normalization_metadata = None #'/mnt/ampa02_data01/tmurakami/model_training/norm_values.pkl'\n",
    "normalization_reference = \"/mnt/ampa02_data01/tmurakami/240417_whole_4color_1st_M037-3pb/fused/fft_norm_2_99p8.zarr\"\n",
    "\n",
    "voxel_size = (1.0,2.0,1.3,1.3) # CZYX\n",
    "\n",
    "corner_positions = [1783,2093,2955]\n",
    "crop_size = [128,512,512]\n",
    "segment_chan = 1\n",
    "reference_chan = 3\n",
    "auto_diam = False # Cellpose automatic diameter estimation.\n",
    "min_size = 40\n",
    "\n",
    "# theoretically, anisotropy parameter affects the accuracy. However in practice, changing this values to be the exact voxel ratio does not significantly add accuracy. \n",
    "# this may be because of the non-isotropic PSF of light-sheet. \n",
    "anisotropy = voxel_size[-1]/voxel_size[1] \n",
    "\n",
    "# Channel parameters which were used during the training.\n",
    "Training_channel = 2 # I do not know but the cellpose see the images as KRGB. If the color is green, set it to 2.\n",
    "Second_training_channel = 1\n",
    "\n",
    "\n",
    "### lazily load images using dask\n",
    "_, ext = os.path.splitext(data_path)\n",
    "imgs = []\n",
    "if ext == '.n5': # n5 assume bigstitcher (bigdataviewer) format\n",
    "    # create Zarr file object\n",
    "    # load images according to the input parameters.\n",
    "    img_zarr = zarr.open(store=zarr.N5Store(data_path), mode='r')\n",
    "    n5_setups = list(img_zarr.keys())\n",
    "    res_list = list(img_zarr[n5_setups[reference_chan]]['timepoint0'].keys())\n",
    "    \n",
    "    for n5_setup in n5_setups:\n",
    "        imgs.append(da.from_zarr(img_zarr[n5_setup]['timepoint0'][res_list[0]]))\n",
    "    imgs = da.stack(imgs)\n",
    "        \n",
    "\n",
    "elif ext == '.zarr': # zarr assumes ome-zarr\n",
    "    # read the image data\n",
    "    store = parse_url(data_path, mode=\"r\").store\n",
    "    reader = Reader(parse_url(data_path))\n",
    "    # nodes may include images, labels etc\n",
    "    nodes = list(reader())\n",
    "    # first node will be the image pixel data\n",
    "    image_node = nodes[0]\n",
    "\n",
    "    dask_data = image_node.data\n",
    "    imgs = dask_data[0]\n",
    "\n",
    "else:\n",
    "    raise ValueError(\"the extension should be .n5 or .zarr\")\n",
    "\n",
    "\n",
    "img_ref = imgs[reference_chan,...].squeeze()#img_zarr[n5_setups[reference_chan]]['timepoint0']['s0']\n",
    "img_ref_ = img_ref[tuple(slice(i,i+j) for i,j in zip(corner_positions, crop_size))].compute()\n",
    "\n",
    "img = imgs[segment_chan,...].squeeze()# img_zarr[n5_setups[segment_chan]]['timepoint0']['s0']\n",
    "img_ = img[tuple(slice(i,i+j) for i,j in zip(corner_positions, crop_size))].compute()\n",
    "img_stack_original = np.stack([img_ref_,img_])\n",
    "\n",
    "if normalization_metadata is not None:\n",
    "    norm_info = pd.read_pickle(normalization_metadata)\n",
    "    img_ref_ = normalization_two_values(img_ref_.astype(float), norm_info[data_path][reference_chan]['lower'], norm_info[data_path][reference_chan]['upper'])\n",
    "    img_ = normalization_two_values(img_.astype(float), norm_info[data_path][segment_chan]['lower'], norm_info[data_path][segment_chan]['upper'])\n",
    "\n",
    "elif normalization_reference is not None:\n",
    "    # read the image data\n",
    "    img_zarr = zarr.open(normalization_reference, mode='r')\n",
    "    scale = img_zarr.attrs['multiscales'][0]['datasets'][0]['coordinateTransformations'][0]['scale']\n",
    "    factors = [i/j for i,j in zip(scale,voxel_size)]\n",
    "\n",
    "    fft = da.from_zarr(img_zarr[0])\n",
    "    fft_corner_positions = [pos//f for pos,f in zip(corner_positions,factors[1:])]\n",
    "    fft_crop_size = [x//f for x,f in zip(crop_size,factors[1:])]\n",
    "\n",
    "    fft_ref_img = fft[reference_chan][tuple(slice(i,i+j) for i,j in zip(fft_corner_positions, fft_crop_size))].compute()\n",
    "    fft_img = fft[segment_chan][tuple(slice(i,i+j) for i,j in zip(fft_corner_positions, fft_crop_size))].compute()\n",
    "\n",
    "\n",
    "    img_ref_ = exposure.match_histograms(img_ref_.astype(np.float32),fft_ref_img)\n",
    "    img_ = exposure.match_histograms(img_.astype(np.float32),fft_img)\n",
    "    \n",
    "img_stack = np.stack([img_ref_,img_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d3ff3a-d0e9-4a12-b2f1-6210a77265d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prediction\n",
    "if (normalization_metadata is not None) or (normalization_reference is not None):\n",
    "    if ~auto_diam:\n",
    "        # with diameter parameter provided without Cellpose normalization\n",
    "        %time masks, flows, styles  = model.eval(img_stack, channels=[Training_channel,Second_training_channel], normalize=False, z_axis=1, diameter=model.diam_mean, do_3D=True, min_size=min_size, progress=True, anisotropy=anisotropy)\n",
    "    else:\n",
    "        %time masks, flows, styles  = model.eval(img_stack, channels=[Training_channel,Second_training_channel], normalize=False, z_axis=1, diameter=None, do_3D=True, min_size=min_size, progress=True, anisotropy=anisotropy)\n",
    "else:\n",
    "    if ~auto_diam:\n",
    "        # with diameter parameter provided \n",
    "        %time masks, flows, styles  = model.eval(img_stack, channels=[Training_channel,Second_training_channel], z_axis=1, diameter=model.diam_mean, do_3D=True, min_size=min_size, progress=True, anisotropy=anisotropy)\n",
    "    else:\n",
    "        # without diameter\n",
    "        %time masks, flows, styles = model.eval(img_stack, channels=[Training_channel,Second_training_channel], z_axis=1, diameter=None, do_3D=True, min_size=min_size, progress=True, anisotropy=anisotropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672f2737-3413-4b4f-8933-721008756f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "viewer = napari.Viewer()\n",
    "viewer.add_image(img_stack, channel_axis=0, name='image01', blending='additive')\n",
    "viewer.add_labels(masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc01698-d6b1-4050-b7c3-ec0f1c435fd4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cellpose2",
   "language": "python",
   "name": "cellpose2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
