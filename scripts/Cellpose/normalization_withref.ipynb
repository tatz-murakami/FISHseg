{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb2dbbe1-0d62-454d-81c0-b89fc90d55c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from skimage import io\n",
    "from tqdm import tqdm\n",
    "import napari\n",
    "import pandas as pd\n",
    "from skimage import exposure\n",
    "\n",
    "import zarr\n",
    "import dask.array as da\n",
    "\n",
    "import pywt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f3c1c1e-b2d3-4d2c-a6f6-57de10415c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load information\n",
    "info_path = '/mnt/ampa02_data01/tmurakami/model_training/info.pkl'\n",
    "obj = pd.read_pickle(info_path)\n",
    "\n",
    "# normalization reference path\n",
    "normalization_references = {\n",
    "    \"/mnt/ampa02_data01/tmurakami/240417_whole_4color_1st_M037-3pb/fused/fused.n5\":\"/mnt/ampa02_data01/tmurakami/240417_whole_4color_1st_M037-3pb/fused/fft_norm_2_99p8.zarr\",\n",
    "    \"/mnt/ampa02_data01/tmurakami/240425_whole_4color_2nd_M037-3pb/fused/fused.n5\":\"/mnt/ampa02_data01/tmurakami/240425_whole_4color_2nd_M037-3pb/fused/fft_norm_2_99p8.zarr\"\n",
    "}\n",
    "\n",
    "# image save path\n",
    "save_path = '/mnt/ampa02_data01/tmurakami/model_training/crops'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e90081fb-4559-4979-92b4-2bbf2cff8761",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 321/321 [30:49<00:00,  5.76s/it]\n"
     ]
    }
   ],
   "source": [
    "resolution = 0 # because the annotation was made using the highest resolution\n",
    "factors = (1.0,4.0,4.0,4.0) # how much time the norm_ref was downsampled.\n",
    "\n",
    "for i in tqdm(range(obj.shape[0])):\n",
    "    data_path = obj.loc[i,'source']\n",
    "    _, ext = os.path.splitext(data_path)\n",
    "    \n",
    "\n",
    "    imgs = []\n",
    "    if ext == '.n5': # n5 assume bigstitcher (bigdataviewer) format\n",
    "        # create Zarr file object\n",
    "        # load images according to the input parameters.\n",
    "        img_zarr = zarr.open(store=zarr.N5Store(data_path), mode='r')\n",
    "        n5_setups = list(img_zarr.keys())\n",
    "        res_list = list(img_zarr[n5_setups[0]]['timepoint0'].keys())\n",
    "\n",
    "        for n5_setup in n5_setups:\n",
    "            imgs.append(da.from_zarr(img_zarr[n5_setup]['timepoint0'][res_list[resolution]]))\n",
    "        imgs = da.stack(imgs)\n",
    "\n",
    "\n",
    "    elif ext == '.zarr': # zarr assumes ome-zarr\n",
    "        # read the image data\n",
    "        store = parse_url(data_path, mode=\"r\").store\n",
    "        reader = Reader(parse_url(data_path))\n",
    "        # nodes may include images, labels etc\n",
    "        nodes = list(reader())\n",
    "        # first node will be the image pixel data\n",
    "        image_node = nodes[0]\n",
    "\n",
    "        dask_data = image_node.data\n",
    "        imgs = dask_data[resolution]\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"the extension should be .n5 or .zarr\")\n",
    "\n",
    "\n",
    "    corner_positions = obj.loc[i,'corner']\n",
    "    crop_size = obj.loc[i,'crop_size']\n",
    "    segment_chan = obj.loc[i,'channel']\n",
    "    reference_chan = obj.loc[i,'ref_channel']\n",
    "    plane_position = int(obj.loc[i,'plane_position'])\n",
    "     \n",
    "    \n",
    "    # load normalization reference\n",
    "    img_norm_zarr = zarr.open(normalization_references[data_path], mode='r')\n",
    "    fft = da.from_zarr(img_norm_zarr[0])\n",
    "\n",
    "    fft_corner_positions = [int(pos//f) for pos,f in zip(corner_positions,factors[1:])]\n",
    "    fft_crop_size = [int(x//f) for x,f in zip(crop_size,factors[1:])]\n",
    "\n",
    "    fft_ref_img = fft[reference_chan][tuple(slice(i,i+j) for i,j in zip(fft_corner_positions, fft_crop_size))].compute()\n",
    "    fft_img = fft[segment_chan][tuple(slice(i,i+j) for i,j in zip(fft_corner_positions, fft_crop_size))].compute()\n",
    "\n",
    "    # load images according to the input parameters.\n",
    "    img_ref = imgs[reference_chan].squeeze()\n",
    "    img_ref_ = img_ref[tuple(slice(i,i+j) for i,j in zip(corner_positions, crop_size))].compute()\n",
    "    img_ref_norm = exposure.match_histograms(img_ref_.astype(np.float32),fft_ref_img)[plane_position,...]\n",
    "\n",
    "    img = imgs[segment_chan].squeeze()\n",
    "    img_ = img[tuple(slice(i,i+j) for i,j in zip(corner_positions, crop_size))].compute()\n",
    "    img_norm = exposure.match_histograms(img_.astype(np.float32),fft_img)[plane_position,...]\n",
    "        \n",
    "    img_stack = np.stack([img_ref_norm,img_norm])\n",
    "    prefix = str(i)\n",
    "    while len(prefix) < 4:\n",
    "        prefix = '0' + prefix\n",
    "        \n",
    "    img_path = os.path.join(save_path, prefix+'_img_norm.tif')\n",
    "    io.imsave(img_path, img_stack, plugin='tifffile', metadata={'axes': 'CYX'}, check_contrast=False)\n",
    "    \n",
    "    # # for debug\n",
    "    # img_path_fft = os.path.join(save_path, prefix+'_img_fft.tif')\n",
    "    # io.imsave(img_path_fft, np.stack([fft_ref_img,fft_img]), plugin='tifffile', metadata={'axes': 'CYX'}, check_contrast=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623f16f3-efa9-49f2-813b-035c6bbd1346",
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cellpose2",
   "language": "python",
   "name": "cellpose2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
